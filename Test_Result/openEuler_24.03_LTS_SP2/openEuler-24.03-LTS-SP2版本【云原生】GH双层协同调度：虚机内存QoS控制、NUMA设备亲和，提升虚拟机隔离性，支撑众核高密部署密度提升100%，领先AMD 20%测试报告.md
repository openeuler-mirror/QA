![avatar](../../images/openEuler.png)

版权所有 © 2023  openEuler社区
 您对“本文档”的复制、使用、修改及分发受知识共享(Creative Commons)署名—相同方式共享4.0国际公共许可协议(以下简称“CC BY-SA 4.0”)的约束。为了方便用户理解，您可以通过访问https://creativecommons.org/licenses/by-sa/4.0/ 了解CC BY-SA 4.0的概要 (但不是替代)。CC BY-SA 4.0的完整协议内容您可以访问如下网址获取：https://creativecommons.org/licenses/by-sa/4.0/legalcode。

修订记录

| 日期      | 修订   版本 | 修改描述                                                     | 作者   |
| --------- | ----------- | ------------------------------------------------------------ | ------ |
| 2025/9/24 | v1.0        | 【云原生】G/H双层协同调度：虚机内存QoS控制、NUMA设备亲和，提升虚拟机隔离性，支撑众核高密部署密度提升100%，领先AMD 20%测试报告 | 鲍超莹 |

关键词： 协同调度，拓扑感知，信息策略感知，众核高密

摘要：本报告主要描述基于openEuler 24.03 SP2版本进行的【云原生】G/H双层协同调度：G/H信息策略相互感知交互，Guest感知硬件topo信息，降低虚拟化损耗，支撑众核高密部署密度提升50%特性的测试过程，报告对测试情况进行说明，对特性的测试充分度进行评估和总结。


缩略语清单：

| 缩略语 | 英文全名   | 中文解释    |
| ------ | ---------- | ----------- |
| G/H    | Guest/Host | 虚拟机/主机 |

# 1     特性概述

描述特性提供的基本能力

# 2     特性测试信息

本节描述被测对象的版本信息和测试的时间及测试轮次，包括依赖的硬件。

| 版本名称            | 测试起始时间 | 测试结束时间 |
| ------------------- | ------------ | ------------ |
| openEuler 24.03 SP2 | 2025/9/8     | 2025/9/12    |

描述特性测试的硬件环境信息

| 硬件型号                                                     | 硬件配置信息              | 备注 |
| ------------------------------------------------------------ | ------------------------- | ---- |
| 鲲鹏920高性能版服务器 160C320T、512G内存、1个25G CX5网卡(每个PF生成80个VF) | BIOS开启SMMU、开启GiCv4.1 |      |
| AMD Zen5 9755 256C512T、1T内存、1个25G CX5网卡               |                           |      |

# 3     测试结论概述

## 3.1   测试整体结论

测试结论可以以一句话描述，如：【云原生】G/H双层协同调度：虚机内存QoS控制、NUMA设备亲和，提升虚拟机隔离性，支撑众核高密部署密度提升100%，领先AMD 20%特性，共计执行6个用例，主要覆盖了功能测试、性能测试和资料测试，整体质量良好

| 测试活动 | 测试子项 | 活动评价 |
| ------- | -------- | ------- |
| 功能测试 | 继承特性测试 | 不涉及 |
| 功能测试 | 新增特性测试 | <font color=green>■</font> |
| 兼容性测试 |          | 不涉及 |
| DFX专项测试 | 性能测试 | <font color=green>■</font> |
| DFX专项测试 | 可靠性/韧性测试 | 不涉及 |
| DFX专项测试 | 安全测试 | 不涉及 |
| 资料测试 | 新增资料测试 | <font color=green>■</font> |
| 其他测试 |         | 不涉及 |

## 3.2   约束说明

当前性能测试基于以下两款硬件型号进行验证，其他硬件型号不承诺性能规格。

鲲鹏服务器规格：鲲鹏920高性能版服务器 160C320T、512G内存、1个25G CX5网卡(每个PF生成80个VF)

鲲鹏服务器BIOS配置：开启SMMU、开启GiCv4.1

鲲鹏服务器内核配置：开启中断直通kvm-arm.vgic_v4_enable=1、1G大页：default_hugepagesz=1G hugepagesz=1G

 

AMD服务器规格：AMD Zen5 9755、256C512T、1T内存、1个25G CX5网卡



## 3.3   遗留问题分析

### 3.3.1 遗留问题影响以及规避措施

| 序号 | 问题单号 | 问题简述 | 问题级别 | 影响分析 | 规避措施 | 历史发现场景 |
| --- | ------- | ------ | ------- | ------- | ------- | ---------- |
|     |         |        |         |         |         |            |
|     |         |        |         |         |         |            |

### 3.3.2 问题统计

|        | 问题总数 | 严重 | 主要 | 次要 | 不重要 |
| ------ | -------- | ---- | ---- | ---- | ------ |
| 数目   |          |      |      |      |        |
| 百分比 |          |      |      |      |        |

# 4 详细测试结论

## 4.1 功能测试
*开源软件：主要关注开源软件升级后的变动点，继承特性由开源软件自带用例保证（需额外关注软件包提供可执行命令、库、服务功能）*
*社区孵化软件：主要参考以下列表*

### 4.1.1 继承特性测试结论

| 序号 | 组件/特性名称 | 特性质量评估 | 备注 |
| --- | ----------- | :--------: | --- |
| |               | <font color=green>■</font> |      |
|      |  | <font color=blue>▲</font> |   |

<font color=red>●</font>： 表示特性不稳定，风险高
<font color=blue>▲</font>： 表示特性基本可用，遗留少量问题
<font color=green>■</font>： 表示特性质量良好

### 4.1.2 新增特性测试结论

| 序号 | 组件/特性名称 | 特性质量评估 | 备注 |
| --- | ----------- | :--------: | --- |
| 1 | 支持内存QoS控制 | <font color=green>■</font> |   |
| 2 | 支持NUMA设备亲和 | <font color=green>■</font> |   |
| 3 | 性能规格：QoS<10%下，众核高密部署密度提升100%，领先AMD 20% | <font color=green>■</font> | |

<font color=red>●</font>： 表示特性不稳定，风险高
<font color=blue>▲</font>： 表示特性基本可用，遗留少量问题
<font color=green>■</font>： 表示特性质量良好

## 4.2 兼容性测试结论

*针对应用软件，主要考虑OS版本兼容性(在不同LTS SPx上的兼容性)、升降级兼容性、上层以来软件兼容性（如升级mysql后，对版本内已发布的使用mysql的软件的兼容性）*

## 4.3 DFX专项测试结论

### 4.3.1 性能测试结论

| 指标大项 | 指标小项 | 指标值 | 测试结论 |
| ------- | ------- | ------ | ------- |
| QoS<10%下，众核高密部署密度提升 | QoS<10%下，众核高密部署密度提升 | 100% | PASS |
| QoS<10%下，众核高密部署密度领先AMD | QoS<10%下，众核高密部署密度领先AMD | 20% | PASS |

### 4.3.2 可靠性/韧性测试结论

| 测试类型 | 测试内容 | 测试结论 |
| ------- | ------- | -------- |
|         |         |          |

### 4.3.3 安全测试结论

| 测试类型 | 测试内容 | 测试结论 |
| ------- | ------- | -------- |
|         |         |          |

## 4.4 资料测试结论
*建议附加资料PR链接*
| 测试类型 | 测试内容 | 测试结论 |
| ------- | ------- | -------- |
| 资料测试 | https://gitee.com/openeuler/Virt-docs/pulls/11 | PASS |

## 4.5 其他测试结论

| 测试类型 | 测试内容 | 测试结论 |
| ------- | ------- | -------- |
|         |         |          |

# 5     测试执行

## 5.1   测试执行统计数据

*本节内容根据测试用例及实际执行情况进行特性整体测试的统计，可根据第二章的测试轮次分开进行统计说明。*

| 版本名称            | 测试用例数 | 用例执行结果 | 发现问题单数 |
| ------------------- | ---------- | ------------ | ------------ |
| openEuler 24.03 SP2 | 6          | PASS         | 0            |

*数据项说明：*

*测试用例数－－到本测试活动结束时，所有可用测试用例数；*

*发现问题单数－－本测试活动总共发现的问题单数。*

## 5.2   测试流程

### 5.2.1 新增特性测试

##### 5.2.1.1 支持内存QoS控制

######  5.2.1.1.1 内存带宽监控

用例描述：

使用 vmtop -G 命令监控虚拟机内存带宽

 预置条件：该特性依赖MPAM，因此需保证主机已挂载resctrl文件系统，虚拟机配置memorytune字段确保resctrl目录下创建了对应虚拟机的控制组

 测试步骤：

1、虚拟机绑定numa，启动后使用测试工具测试虚拟机内存带宽

2、主机侧使用 vmtop -G 命令查看虚拟机内存带宽

 预期结果：

vmtop -G 命令回显能正常刷新显示虚拟机实时内存带宽



######  5.2.1.1.2 内存带宽配置

用例1描述：

bandwidth：内存带宽上限控制，取值范围[0,100]；

hardlimit: 取值范围0或1，当MBHDL=1时，限制MB共享资源使用量不能超出MB设置值即bandwidth，若MBHDL=0，则允许空闲情况下，MB共享资源使用量超过MB设置值

 预置条件：主机已挂载resctrl文件系统，虚拟机绑numa，虚拟机规格相同

 测试步骤：

1、未配置bandwidth参数，启动虚拟机，测试虚拟机内存带宽

2、配置bandwidth为10，hardlimi为1，启动虚拟机，测试虚拟机内存带宽

 预期结果：

1、查看/sys/fs/resctrl/qemu-xxxx/schemata文件确认配置生效

2、配置bandwidth为10时虚拟机内存带宽小于未配置该字段时的虚拟机带宽



用例2描述：

min_bandwidth: 内存带宽限低，当共享资源实际占比低于设置值，会自动提高对该资源使用优先级，取值范围[0-100]

 预置条件：主机已挂载resctrl文件系统，虚拟机绑numa，虚拟机规格相同

 测试步骤：

1、一台虚拟机不配置min_bandwidth为90（两台同等规格虚拟机同时竞争资源建议配置min_bandwidth超过50，n台同等规格虚拟机竞争时建议配置超过100/n）

2、一台虚拟机未配置min_bandwidth

3、两台虚拟机同时加压测试内存带宽

 预期结果：

1、查看/sys/fs/resctrl/qemu-xxxx/schemata文件确认配置生效

2、配置min_bandwidth为90的虚拟机内存带宽大于未配置该字段的虚拟机带宽，可通过vmtop -G命令监控虚拟机带宽



用例3描述：

设计描述：

priority: 优先级配置，设置越大，优先级越高，反之，数字越小，优先级越低。MBPRI默认值为3，合法值范围[0,7]

预置条件：主机已挂载resctrl文件系统，虚拟机绑numa，虚拟机规格相同

测试步骤：

1、一台虚拟机配置priority为7

2、一台虚拟机未配置priority，默认为3

3、两台虚拟机同时加压测试内存带宽

预期结果：

1、查看/sys/fs/resctrl/qemu-xxxx/schemata文件确认配置生效

2、配置priority为7的虚拟机内存带宽大于未配置该字段的虚拟机带宽，可通过vmtop -G命令监控虚拟机带宽

##### 5.2.1.2 支持NUMA设备亲和

用例描述：

虚拟机xml配置直通设备numa亲和，在虚拟机内通过sysfs接口查看设备numa信息。

 预置条件：vfio直通设备

 测试步骤：

1、虚拟机xml配置直通设备numa信息:

  <hostdev mode='subsystem' type='pci' managed='yes'>

  <driver name='vfio'/>

  <source>

​    <address domain='0x0000' bus='0x03' slot='0x10' function='0x00'/>

  </source>

  <numa node='0'>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

  </hostdev>

 2、在虚拟机内通过sysfs接口(cat /sys/bus/pci/devices/bdf/numa_node)查看对应直通设备的numa信息

 

预期结果：

1、虚拟机内sysfs接口获取的直通设备所在numa不是非法值，且与xml配置的一致



##### 5.2.1.3 性能规格：QoS<10%下，众核高密部署密度提升100%，领先AMD 20%

用例描述：

对比QoS<10%下，众核高密部署密度能否提升100%，领先AMD 20%



 预置条件：

服务器规格：鲲鹏920高性能版服务器 160C320T、512G内存、1个25G CX5网卡(每个PF生成80个VF)

服务器BIOS配置：开启SMMU、开启GiCv4.1

服务器内核配置：开启中断直通kvm-arm.vgic_v4_enable=1、1G大页：default_hugepagesz=1G hugepagesz=1G

 

AMD服务器规格：AMD Zen5 9755、256C512T、1T内存、1个25G CX5网卡



 测试步骤：

1、创建启动2台虚机，单虚机2个numa，160U，240G内存，40个VF直通网卡

2、每个虚拟机里面跑2U8G的openEuler22.03版本 Docker容器

3、在每个容器中部署redis-server，使用多个服务器作为client端，启动redis-benchmark对redis-server进行压测，观测其中一个容器中redis业务对应p99时延指标。为消除client端多个redis-benchmark之间的相互干扰，观测容器使用独立client服务器进行压测。容器中redis-server进程绑定其中一个核，网卡中断绑定另一个核。

4、测试模型：

redis-benchmark -h $IP -p 6379 -c 40 -d 3 -n 2000000 -r 10000000 -t get --threads 50

模型参数说明：

-h <hostname> 容器ip

-p <port> 服务器的端口号（默认值为6379）

-c <clients> 并发的连接数量（默认值为50）

-n <requests> 发出的请求总数（默认值为100000）

-d <size> SET/GET命令所操作的值的数据大小，以字节为单位（默认值为2）

-r <keyspacelen> 随机键个数

-t <tests> 测试的命令

--threads n 每个连接的线程数

 

预期结果：

QoS<10%下，众核高密部署密度提升100%，领先AMD 20%





# 6     附件

裸机基线数据：

![image-20250924150009633](C:\Users\b00836800\AppData\Roaming\Typora\typora-user-images\image-20250924150009633.png)



AMD Zen5数据：

![image-20250924150022973](C:\Users\b00836800\AppData\Roaming\Typora\typora-user-images\image-20250924150022973.png)



优化后数据：

![image-20250924150051313](C:\Users\b00836800\AppData\Roaming\Typora\typora-user-images\image-20250924150051313.png)

 





**结论：**

鲲鹏920高性能版服务器在部署28容器时，观测容器的p99时延相比于单示例已劣化超过10%；AMD Zen5服务器在相同测试方法下，p99时延qos<10%的条件下，容器个数最大为48。经过优化，鲲鹏920高性能版服务器在p99时延qos<10%的条件下，容器个数达到60+，达到相较于鲲鹏920高性能版服务器基线提升100%的目标，同时达到相较于AMD Zen5领先20%目标。





 



 

 